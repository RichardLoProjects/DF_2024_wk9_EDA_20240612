{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Digital Futures Week 9**\n",
    "### **Supplementary Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **0. Introduction**\n",
    "\n",
    "Lisa is upset that she has no time to talk about data analysis in Python. The task is to cheer her up with a notebook to cover topics on: Pandas, EDA (exploratory data analysis), and visualisation. Hopefully these skills will prove useful when applied to the capstone project and further Data Engineering endeavors. The learning experience will be generated from the following set of instructions:\n",
    "\n",
    "1. Find and download data to work with\n",
    "2. Identify and import useful Python libraries\n",
    "3. Extract data\n",
    "4. Transform and clean data\n",
    "5. Load data into the \"database\" (pickles will be our \"database-at-home\")\n",
    "6. EDA and visulisation\n",
    "7. Business Insight\n",
    "8. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1. Obtain Data**\n",
    "\n",
    "To obtain some data, we searched Kaggle for data sets. The search yielded these results:\n",
    "- [Customer Churn Records](<https://www.kaggle.com/datasets/radheshyamkollipara/bank-customer-churn>)\n",
    "- [Bankruptcy Prediction](<https://www.kaggle.com/datasets/fedesoriano/company-bankruptcy-prediction/data>)\n",
    "- [Options Trading Data](<https://www.kaggle.com/datasets/bendgame/options-market-trades>)\n",
    "\n",
    "The data was downloaded and stored locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2. Python Libraries**\n",
    "\n",
    "Pandas was a mandatory requirement to explore. The rest of the libraries seem like they could prove useful for EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **3. ETL - Extract**\n",
    "\n",
    "Extraction phase of the data pipeline was executed on the Bankruptcy data set. Retrospectively, running `print(df.head())` was not a good idea. We refer to Lisa's example of the Sky database with 15 thousand columns and 27 million rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('data/CompanyBankruptcyInfo.csv')\n",
    "    print(f'Dataframe loaded successfully! It has {df.shape[1]} columns and {df.shape[0]} rows.')\n",
    "    print(f'It is a {type(df)} data type in Python.')\n",
    "    # print(df.head())\n",
    "except:\n",
    "    print('What are you doing??')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outside of `df.head()` and `df.tail()`, I learned that there are other ways to make and initial EDA.\n",
    "\n",
    "- `df.shape` returns a tuple featuring, respectively, the number of rows and number of columns.\n",
    "- `df.dtypes` returns a new table of column names and column data types.\n",
    "- `df.columns` returns **ALL** columns - beware!\n",
    "- `df.describe()` prints summary statistics of **ALL** columns - beware!\n",
    "- `df.info()` prints... some info? And also amount of memory used.\n",
    "\n",
    "Uncomment the line you wish to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.shape)\n",
    "# print(df.dtypes)\n",
    "# print(df.columns)\n",
    "# df.describe()\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **4. ETL - Transform**\n",
    "\n",
    "In real business, data cleaning is an essential step to transition unstructured or semi-structured data into a format suitable for structured data. Common steps within data cleaning might include\n",
    "\n",
    "- Handling or deletion of `NULL` values\n",
    "- Identifying and removing duplicate data\n",
    "- Enforcing consistency: dtypes, formatting (dates, capitalisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.dropna().drop_duplicates()\n",
    "message = 'The table remains unchanged.' if df_clean.shape == df.shape else 'Table was cleaned.'\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above Jupyter cell we attempted to clean the data set by removing `NULL` values and then removing duplicate data. We also verified that the original dataset had no `NULL` values or duplicates and hence the dataframe remains unchanged. I will need Lisa's expert advice on where to get started on further data cleaning (such as enforcing consistency) when I am unable to view the table such as the case where the table has shape `(27 x 10^6, 15 x 10^3)`. Lisa, please help! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **5. ETL - Load**\n",
    "\n",
    "To explain my choice in database, please allow me to present the following dialogue.\n",
    "\n",
    "- Child: Mom, can we use the PostgreSQL database?\n",
    "- Mom: We have the database at home.\n",
    "- The database at home: `import pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('database/bankruptcy_db.pkl', 'wb') as file:\n",
    "    pkl.dump(df_clean, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all seriousness, I believe there are proper ways to load dataframes into a database. A quick Google search yields [this](<https://www.codingforentrepreneurs.com/shorts/export-pandas-dataframe-to-a-postgresql-database-table/>). My decision to not use a database stems from the fear of incurring cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **6. EDA and Visualisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('database/bankruptcy_db.pkl', 'rb') as file:\n",
    "    db:pd.DataFrame = pkl.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would assume EDA involves looking at the shape of the database and the summary statistics. Please correct me if I am wrong or have omitted something important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dataframe loaded successfully! It has {db.shape[1]} columns and {db.shape[0]} rows.')\n",
    "db.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNFINISHED VISUALISATION (S6) AND s7/s8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **7. Business Interpretation**\n",
    "\n",
    "Hypothesis: Low operating profit results in bankruptcy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **8. Conclusion**\n",
    "\n",
    "It appears that Pandas is Python's natural tool for handling tabular data sets, and Kaggle is a great source of educational data. From a perspective of a Data Engineer, they can use Pandas to construct data pipelines to ETL data from csv files into databases. Moreover, Pandas has built in methods for initial EDA by answering basic questions like\n",
    "\n",
    "- How big is my table?\n",
    "- What are the data types of my columns?\n",
    "- What are the summary statistics of my data?\n",
    "\n",
    "Visualisation with Matplotlib might also help businesses make data driven decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
